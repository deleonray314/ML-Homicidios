# üöÄ Gu√≠a R√°pida: Ejecutar ETL del Data Lake

## ‚úÖ Requisitos

1. ‚úÖ Docker corriendo (Data Lake, Data Warehouse, Adminer)
2. ‚úÖ Esquemas de base de datos creados
3. ‚úÖ C√≥digo ETL implementado

## üéØ Comandos de Ejecuci√≥n

### Carga Inicial Completa

```bash
python scripts/load_datalake.py --initial
```

### Carga Incremental

```bash
python scripts/load_datalake.py --incremental
```

### Cargar Dataset Espec√≠fico

```bash
# Departamentos (33 registros)
python scripts/load_datalake.py --dataset departamentos

# Municipios (~1100 registros)
python scripts/load_datalake.py --dataset municipios

# Homicidios (todos los registros hist√≥ricos)
python scripts/load_datalake.py --dataset homicidios --initial
```

## üîç Verificar Datos en Adminer

1. Abre: http://localhost:8080
2. Conecta:
   - Sistema: PostgreSQL
   - Servidor: `datalake`
   - Usuario: `datalake_user`
   - Contrase√±a: `datalake_password_2024`
   - Base de datos: `homicidios_datalake`
3. Explora las tablas:
   - `raw_homicidios`
   - `raw_divipola_departamentos`
   - `raw_divipola_municipios`
   - `data_load_log`

## ‚ö†Ô∏è Troubleshooting

**Error: Connection refused**

- Soluci√≥n: Reinicia Docker con `docker-compose down && docker-compose up -d`

**Error: No module named 'src'**

- Soluci√≥n: Ejecuta desde la ra√≠z del proyecto, no desde `src/`

**Error: API timeout**

- Soluci√≥n: Verifica tu conexi√≥n a internet
- La API de Datos Abiertos puede estar lenta

## üìù Pr√≥ximos Pasos

Despu√©s de cargar los datos:

1. Verificar en Adminer que los datos se cargaron
2. Implementar ETL del Data Warehouse (transformaci√≥n al modelo estrella)
3. Crear dashboards en Streamlit
