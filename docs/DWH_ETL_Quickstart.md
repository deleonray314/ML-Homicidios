# ğŸš€ GuÃ­a RÃ¡pida: ETL Data Warehouse

## ğŸ“‹ Resumen

El ETL del Data Warehouse transforma datos del Data Lake (raw) al Data Warehouse (modelo estrella) para anÃ¡lisis y ML.

---

## ğŸ—ï¸ Arquitectura

```
Data Lake (Raw)          ETL Transform          Data Warehouse (Star Schema)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ raw_homicidiosâ”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ Dimensiones  â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ dim_fecha            â”‚
â”‚ raw_divipola  â”‚        â”‚ + Hechos     â”‚        â”‚ dim_departamento     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ dim_municipio        â”‚
                                                 â”‚ dim_sexo             â”‚
                                                 â”‚ fact_homicidios      â”‚
                                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Carga Inicial (Primera Vez)

```bash
# 1. Asegurarse que Data Lake tiene datos
docker exec ml-homicidios-etl-cron python scripts/load_datalake.py --initial

# 2. Ejecutar carga inicial del DWH
docker exec ml-homicidios-etl-cron python scripts/load_datawarehouse.py --initial
```

**Esto carga:**
- âœ… `dim_departamento` (33 registros)
- âœ… `dim_municipio` (1,121 registros)
- âœ… `dim_sexo` (3 registros)
- âœ… `dim_fecha` (~7,000 fechas)
- âœ… `fact_homicidios` (~332,000 homicidios)

---

## ğŸ”„ Carga Incremental (AutomÃ¡tica)

### **Cron Jobs Configurados:**

| Tarea | DÃ­a | Hora | DescripciÃ³n |
|-------|-----|------|-------------|
| **Data Lake â†’ API** | Viernes | 23:00 | Extrae nuevos homicidios |
| **DWH â† Data Lake** | SÃ¡bado | 01:00 | Transforma a modelo estrella |
| **Catch-up Data Lake** | Diario | 08:00 | Verifica cargas perdidas |
| **Catch-up DWH** | Diario | 09:00 | Verifica cargas perdidas DWH |

### **Flujo Semanal:**
```
Viernes 23:00  â†’ API â†’ Data Lake (raw_homicidios)
SÃ¡bado  01:00  â†’ Data Lake â†’ DWH (fact_homicidios)
```

---

## ğŸ§ª Pruebas Manuales

### **Ejecutar carga incremental manualmente:**

```bash
# DWH incremental
docker exec ml-homicidios-etl-cron python scripts/load_datawarehouse.py --incremental
```

### **Verificar catch-up DWH:**

```bash
docker exec ml-homicidios-etl-cron python scripts/catchup_check_dwh.py
```

### **Ver logs:**

```bash
# Logs del ETL DWH
docker exec ml-homicidios-etl-cron tail -f /app/logs/cron_dwh.log

# Logs de catch-up DWH
docker exec ml-homicidios-etl-cron tail -f /app/logs/catchup_dwh.log
```

---

## ğŸ“Š Verificar Datos en DWH

### **Conectar a Adminer:**
- URL: http://localhost:8080
- Sistema: PostgreSQL
- Servidor: `datawarehouse`
- Usuario: `dw_user`
- ContraseÃ±a: `dw_password_2024`
- Base de datos: `homicidios_dw`

### **Queries de VerificaciÃ³n:**

```sql
-- Contar registros en dimensiones
SELECT 'dim_fecha' as tabla, COUNT(*) as registros FROM dim_fecha
UNION ALL
SELECT 'dim_departamento', COUNT(*) FROM dim_departamento
UNION ALL
SELECT 'dim_municipio', COUNT(*) FROM dim_municipio
UNION ALL
SELECT 'dim_sexo', COUNT(*) FROM dim_sexo
UNION ALL
SELECT 'fact_homicidios', COUNT(*) FROM fact_homicidios;

-- Ver Ãºltimas cargas ETL
SELECT * FROM etl_log ORDER BY completed_at DESC LIMIT 5;

-- Homicidios por departamento
SELECT * FROM v_homicidios_por_departamento LIMIT 10;

-- Homicidios por mes
SELECT * FROM v_homicidios_por_mes ORDER BY aÃ±o DESC, mes DESC LIMIT 12;
```

---

## ğŸ” Monitoreo

### **Ver estado del ETL:**

```bash
# Ver todas las cargas ETL
docker exec ml-homicidios-datawarehouse psql -U dw_user -d homicidios_dw -c "SELECT process_name, records_processed, status, completed_at FROM etl_log ORDER BY completed_at DESC LIMIT 10;"
```

---

## ğŸ› ï¸ Troubleshooting

### **Problema: No hay datos en DWH**

```bash
# Verificar que Data Lake tiene datos
docker exec ml-homicidios-datalake psql -U datalake_user -d homicidios_datalake -c "SELECT COUNT(*) FROM raw_homicidios;"

# Ejecutar carga inicial
docker exec ml-homicidios-etl-cron python scripts/load_datawarehouse.py --initial
```

### **Problema: Carga incremental no detecta nuevos datos**

```bash
# Ver Ãºltima carga en DWH
docker exec ml-homicidios-datawarehouse psql -U dw_user -d homicidios_dw -c "SELECT MAX(loaded_at) FROM fact_homicidios;"

# Ver Ãºltima carga en Data Lake
docker exec ml-homicidios-datalake psql -U datalake_user -d homicidios_datalake -c "SELECT MAX(loaded_at) FROM raw_homicidios;"
```

---

## ğŸ“ Archivos Creados

```
src/data_warehouse/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ dwh_connection.py          # ConexiÃ³n al DWH
â””â”€â”€ dwh_etl_loader.py          # LÃ³gica ETL completa

scripts/
â”œâ”€â”€ load_datawarehouse.py      # Script principal ETL
â””â”€â”€ catchup_check_dwh.py       # VerificaciÃ³n de cargas perdidas

docker/
â””â”€â”€ crontab                    # Cron jobs actualizados
```

---

## âœ… Checklist de VerificaciÃ³n

- [ ] Data Lake tiene datos (`raw_homicidios`, `raw_divipola_*`)
- [ ] Carga inicial DWH ejecutada
- [ ] Dimensiones pobladas (departamento, municipio, sexo, fecha)
- [ ] Tabla de hechos poblada (`fact_homicidios`)
- [ ] Cron jobs configurados
- [ ] Catch-up automÃ¡tico funciona
- [ ] Logs se generan correctamente

---

Â¡El ETL del Data Warehouse estÃ¡ listo! ğŸ‰
